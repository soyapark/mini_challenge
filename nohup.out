Starting training
Current learning rate: 0.001
[1:0.10] loss: 4.681
[1:0.20] loss: 4.634
[1:0.30] loss: 4.608
[1:0.40] loss: 4.584
[1:0.50] loss: 4.572
[1:0.60] loss: 4.556
[1:0.70] loss: 4.541
[1:0.80] loss: 4.534
[1:0.90] loss: 4.518
[1:1.00] loss: 4.506
Validating ... 
The current model1:
Top 1 Accuracy: 0.027899997
Top 5 Accuracy: 0.12399998
Current learning rate: 0.001
[2:0.10] loss: 4.495
[2:0.20] loss: 4.482
[2:0.30] loss: 4.476
[2:0.40] loss: 4.460
[2:0.50] loss: 4.452
[2:0.60] loss: 4.447
[2:0.70] loss: 4.437
[2:0.80] loss: 4.431
[2:0.90] loss: 4.419
[2:1.00] loss: 4.401
Validating ... 
The current model2:
Top 1 Accuracy: 0.044300005
Top 5 Accuracy: 0.16489998
Current learning rate: 0.001
[3:0.10] loss: 4.394
[3:0.20] loss: 4.391
[3:0.30] loss: 4.371
[3:0.40] loss: 4.372
[3:0.50] loss: 4.358
[3:0.60] loss: 4.347
[3:0.70] loss: 4.333
[3:0.80] loss: 4.331
[3:0.90] loss: 4.313
[3:1.00] loss: 4.300
Validating ... 
The current model3:
Top 1 Accuracy: 0.053399995
Top 5 Accuracy: 0.19780001
Current learning rate: 0.001
[4:0.10] loss: 4.288
[4:0.20] loss: 4.285
[4:0.30] loss: 4.276
[4:0.40] loss: 4.257
[4:0.50] loss: 4.249
[4:0.60] loss: 4.242
[4:0.70] loss: 4.235
[4:0.80] loss: 4.214
[4:0.90] loss: 4.220
[4:1.00] loss: 4.203
Validating ... 
The current model4:
Top 1 Accuracy: 0.0646
Top 5 Accuracy: 0.22640002
Current learning rate: 0.001
[5:0.10] loss: 4.191
[5:0.20] loss: 4.176
[5:0.30] loss: 4.168
[5:0.40] loss: 4.143
[5:0.50] loss: 4.154
[5:0.60] loss: 4.146
[5:0.70] loss: 4.122
[5:0.80] loss: 4.128
[5:0.90] loss: 4.109
[5:1.00] loss: 4.111
Validating ... 
The current model5:
Top 1 Accuracy: 0.077999994
Top 5 Accuracy: 0.25390002
Current learning rate: 0.001
[6:0.10] loss: 4.084
[6:0.20] loss: 4.079
[6:0.30] loss: 4.072
[6:0.40] loss: 4.071
[6:0.50] loss: 4.072
[6:0.60] loss: 4.046
[6:0.70] loss: 4.068
[6:0.80] loss: 4.044
[6:0.90] loss: 4.034
[6:1.00] loss: 4.015
Validating ... 
The current model6:
Top 1 Accuracy: 0.086399995
Top 5 Accuracy: 0.2735
Current learning rate: 0.001
[7:0.10] loss: 4.023
[7:0.20] loss: 4.007
[7:0.30] loss: 3.997
[7:0.40] loss: 3.996
[7:0.50] loss: 3.985
[7:0.60] loss: 3.992
[7:0.70] loss: 3.970
[7:0.80] loss: 3.995
[7:0.90] loss: 3.962
[7:1.00] loss: 3.967
Validating ... 
The current model7:
Top 1 Accuracy: 0.0979
Top 5 Accuracy: 0.29029998
Current learning rate: 0.001
[8:0.10] loss: 3.954
[8:0.20] loss: 3.940
[8:0.30] loss: 3.954
[8:0.40] loss: 3.937
[8:0.50] loss: 3.940
[8:0.60] loss: 3.929
[8:0.70] loss: 3.918
[8:0.80] loss: 3.913
[8:0.90] loss: 3.897
[8:1.00] loss: 3.903
Validating ... 
The current model8:
Top 1 Accuracy: 0.1016
Top 5 Accuracy: 0.30479997
Current learning rate: 0.001
[9:0.10] loss: 3.896
[9:0.20] loss: 3.896
[9:0.30] loss: 3.894
[9:0.40] loss: 3.875
[9:0.50] loss: 3.880
[9:0.60] loss: 3.861
[9:0.70] loss: 3.859
[9:0.80] loss: 3.870
[9:0.90] loss: 3.862
[9:1.00] loss: 3.842
Validating ... 
The current model9:
Top 1 Accuracy: 0.11049999
Top 5 Accuracy: 0.32359996
Current learning rate: 0.001
[10:0.10] loss: 3.846
[10:0.20] loss: 3.840
[10:0.30] loss: 3.843
[10:0.40] loss: 3.835
[10:0.50] loss: 3.813
[10:0.60] loss: 3.812
[10:0.70] loss: 3.817
[10:0.80] loss: 3.820
[10:0.90] loss: 3.808
[10:1.00] loss: 3.797
Validating ... 
The current model10:
Top 1 Accuracy: 0.11669999
Top 5 Accuracy: 0.3346
Training terminated
Starting training
************ Running optimization 0
Current learning rate: 0.001
Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f11abc79320>>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 399, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 378, in _shutdown_workers
    self.worker_result_queue.get()
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py", line 337, in get
    return _ForkingPickler.loads(res)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 151, in rebuild_storage_fd
    fd = df.detach()
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 493, in Client
    answer_challenge(c, authkey)
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 737, in answer_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 155, in <module>
    run(i)
  File "train.py", line 64, in run
    outputs = model(inputs)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/test/mini_challenge/models/ResNet.py", line 124, in forward
    output = self.layer2(output)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/test/mini_challenge/models/ResNet.py", line 72, in forward
    output = self.Conv3(output)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA error: out of memory
Starting training
************ Running optimization 0
Current learning rate: 0.001
[1:0.10] loss: 4.712
[1:0.20] loss: 4.635
[1:0.30] loss: 4.606
[1:0.40] loss: 4.590
[1:0.50] loss: 4.577
[1:0.60] loss: 4.565
[1:0.70] loss: 4.548
[1:0.80] loss: 4.545
[1:0.90] loss: 4.532
[1:1.00] loss: 4.523
Validating ... 
The current model1:
Top 1 Accuracy: 0.021300001
Top 5 Accuracy: 0.09890001
Current learning rate: 0.001
[2:0.10] loss: 4.506
[2:0.20] loss: 4.497
[2:0.30] loss: 4.484
[2:0.40] loss: 4.482
[2:0.50] loss: 4.467
[2:0.60] loss: 4.458
[2:0.70] loss: 4.453
[2:0.80] loss: 4.441
[2:0.90] loss: 4.434
[2:1.00] loss: 4.419
Validating ... 
The current model2:
Top 1 Accuracy: 0.0309
Top 5 Accuracy: 0.1292
Current learning rate: 0.001
[3:0.10] loss: 4.418
[3:0.20] loss: 4.404
[3:0.30] loss: 4.392
[3:0.40] loss: 4.379
[3:0.50] loss: 4.378
[3:0.60] loss: 4.374
[3:0.70] loss: 4.364
[3:0.80] loss: 4.358
[3:0.90] loss: 4.340
[3:1.00] loss: 4.339
Validating ... 
The current model3:
Top 1 Accuracy: 0.034999996
Top 5 Accuracy: 0.156
Current learning rate: 0.001
[4:0.10] loss: 4.316
[4:0.20] loss: 4.328
[4:0.30] loss: 4.306
[4:0.40] loss: 4.314
[4:0.50] loss: 4.304
[4:0.60] loss: 4.286
[4:0.70] loss: 4.281
[4:0.80] loss: 4.263
[4:0.90] loss: 4.268
[4:1.00] loss: 4.257
Validating ... 
The current model4:
Top 1 Accuracy: 0.04989999
Top 5 Accuracy: 0.1925
Current learning rate: 0.001
[5:0.10] loss: 4.254
[5:0.20] loss: 4.228
[5:0.30] loss: 4.227
[5:0.40] loss: 4.218
[5:0.50] loss: 4.220
[5:0.60] loss: 4.205
[5:0.70] loss: 4.198
[5:0.80] loss: 4.185
[5:0.90] loss: 4.182
[5:1.00] loss: 4.162
Validating ... 
The current model5:
Top 1 Accuracy: 0.0625
Top 5 Accuracy: 0.2232
Current learning rate: 0.001
[6:0.10] loss: 4.143
[6:0.20] loss: 4.143
[6:0.30] loss: 4.134
[6:0.40] loss: 4.105
[6:0.50] loss: 4.127
[6:0.60] loss: 4.107
[6:0.70] loss: 4.109
[6:0.80] loss: 4.090
[6:0.90] loss: 4.066
[6:1.00] loss: 4.062
Validating ... 
The current model6:
Top 1 Accuracy: 0.0808
Top 5 Accuracy: 0.2538
Current learning rate: 0.001
[7:0.10] loss: 4.059
[7:0.20] loss: 4.034
[7:0.30] loss: 4.035
[7:0.40] loss: 4.009
[7:0.50] loss: 4.033
[7:0.60] loss: 3.983
[7:0.70] loss: 3.994
[7:0.80] loss: 3.998
[7:0.90] loss: 3.975
[7:1.00] loss: 3.967
Validating ... 
The current model7:
Top 1 Accuracy: 0.0837
Top 5 Accuracy: 0.27040002
Current learning rate: 0.001
[8:0.10] loss: 3.961
[8:0.20] loss: 3.964
[8:0.30] loss: 3.938
[8:0.40] loss: 3.936
[8:0.50] loss: 3.918
[8:0.60] loss: 3.929
[8:0.70] loss: 3.910
[8:0.80] loss: 3.908
[8:0.90] loss: 3.894
[8:1.00] loss: 3.903
Validating ... 
The current model8:
Top 1 Accuracy: 0.099899985
Top 5 Accuracy: 0.2969
Current learning rate: 0.001
[9:0.10] loss: 3.884
[9:0.20] loss: 3.877
[9:0.30] loss: 3.864
[9:0.40] loss: 3.876
[9:0.50] loss: 3.870
[9:0.60] loss: 3.849
[9:0.70] loss: 3.860
[9:0.80] loss: 3.845
[9:0.90] loss: 3.831
[9:1.00] loss: 3.840
Validating ... 
The current model9:
Top 1 Accuracy: 0.1069
Top 5 Accuracy: 0.31509998
Current learning rate: 0.001
[10:0.10] loss: 3.814
[10:0.20] loss: 3.811
[10:0.30] loss: 3.801
[10:0.40] loss: 3.811
[10:0.50] loss: 3.791
[10:0.60] loss: 3.802
[10:0.70] loss: 3.795
[10:0.80] loss: 3.781
[10:0.90] loss: 3.808
[10:1.00] loss: 3.792
Validating ... 
The current model10:
Top 1 Accuracy: 0.11359999
Top 5 Accuracy: 0.32779998
Current learning rate: 0.001
[11:0.10] loss: 3.761
[11:0.20] loss: 3.776
[11:0.30] loss: 3.755
[11:0.40] loss: 3.755
[11:0.50] loss: 3.735
[11:0.60] loss: 3.750
[11:0.70] loss: 3.741
[11:0.80] loss: 3.733
[11:0.90] loss: 3.731
[11:1.00] loss: 3.732
Validating ... 
The current model11:
Top 1 Accuracy: 0.11869999
Top 5 Accuracy: 0.3372
Current learning rate: 0.001
[12:0.10] loss: 3.724
[12:0.20] loss: 3.713
[12:0.30] loss: 3.707
[12:0.40] loss: 3.708
[12:0.50] loss: 3.687
[12:0.60] loss: 3.712
[12:0.70] loss: 3.704
[12:0.80] loss: 3.672
[12:0.90] loss: 3.688
[12:1.00] loss: 3.678
Validating ... 
The current model12:
Top 1 Accuracy: 0.12629999
Top 5 Accuracy: 0.354
Current learning rate: 0.001
[13:0.10] loss: 3.689
[13:0.20] loss: 3.654
[13:0.30] loss: 3.666
[13:0.40] loss: 3.656
[13:0.50] loss: 3.640
[13:0.60] loss: 3.655
[13:0.70] loss: 3.652
[13:0.80] loss: 3.646
[13:0.90] loss: 3.640
[13:1.00] loss: 3.642
Validating ... 
The current model13:
Top 1 Accuracy: 0.13159999
Top 5 Accuracy: 0.36559993
Current learning rate: 0.001
[14:0.10] loss: 3.626
[14:0.20] loss: 3.626
[14:0.30] loss: 3.620
[14:0.40] loss: 3.594
[14:0.50] loss: 3.626
[14:0.60] loss: 3.615
[14:0.70] loss: 3.600
[14:0.80] loss: 3.603
[14:0.90] loss: 3.592
[14:1.00] loss: 3.608
Validating ... 
The current model14:
Top 1 Accuracy: 0.13599998
Top 5 Accuracy: 0.3781
Current learning rate: 0.001
[15:0.10] loss: 3.556
[15:0.20] loss: 3.584
[15:0.30] loss: 3.587
[15:0.40] loss: 3.568
[15:0.50] loss: 3.563
[15:0.60] loss: 3.578
[15:0.70] loss: 3.570
[15:0.80] loss: 3.558
[15:0.90] loss: 3.570
[15:1.00] loss: 3.560
Validating ... 
The current model15:
Top 1 Accuracy: 0.14469999
Top 5 Accuracy: 0.3842
Current learning rate: 0.001
[16:0.10] loss: 3.550
[16:0.20] loss: 3.529
[16:0.30] loss: 3.545
[16:0.40] loss: 3.536
[16:0.50] loss: 3.550
[16:0.60] loss: 3.522
[16:0.70] loss: 3.544
[16:0.80] loss: 3.510
[16:0.90] loss: 3.512
[16:1.00] loss: 3.504
Validating ... 
The current model16:
Top 1 Accuracy: 0.1492
Top 5 Accuracy: 0.40000004
Current learning rate: 0.001
[17:0.10] loss: 3.482
[17:0.20] loss: 3.501
[17:0.30] loss: 3.530
[17:0.40] loss: 3.499
[17:0.50] loss: 3.484
[17:0.60] loss: 3.481
[17:0.70] loss: 3.483
[17:0.80] loss: 3.507
[17:0.90] loss: 3.510
[17:1.00] loss: 3.464
Validating ... 
The current model17:
Top 1 Accuracy: 0.1526
Top 5 Accuracy: 0.4055
Current learning rate: 0.001
[18:0.10] loss: 3.462
[18:0.20] loss: 3.460
[18:0.30] loss: 3.456
[18:0.40] loss: 3.465
[18:0.50] loss: 3.462
[18:0.60] loss: 3.462
[18:0.70] loss: 3.442
[18:0.80] loss: 3.468
[18:0.90] loss: 3.434
[18:1.00] loss: 3.442
Validating ... 
The current model18:
Top 1 Accuracy: 0.16039999
Top 5 Accuracy: 0.42409995
Current learning rate: 0.001
[19:0.10] loss: 3.441
[19:0.20] loss: 3.423
[19:0.30] loss: 3.418
[19:0.40] loss: 3.435
[19:0.50] loss: 3.430
[19:0.60] loss: 3.443
[19:0.70] loss: 3.397
[19:0.80] loss: 3.412
[19:0.90] loss: 3.406
[19:1.00] loss: 3.399
Validating ... 
The current model19:
Top 1 Accuracy: 0.16640002
Top 5 Accuracy: 0.43
Current learning rate: 0.001
[20:0.10] loss: 3.388
[20:0.20] loss: 3.384
[20:0.30] loss: 3.405
[20:0.40] loss: 3.395
[20:0.50] loss: 3.397
[20:0.60] loss: 3.360
[20:0.70] loss: 3.374
[20:0.80] loss: 3.387
[20:0.90] loss: 3.384
[20:1.00] loss: 3.374
Validating ... 
The current model20:
Top 1 Accuracy: 0.16680002
Top 5 Accuracy: 0.4283
************ Running optimization 1
Current learning rate: 0.001
  File "train.py", line 44
    optimizer = optim.SGD(list(filter(lambda p: p.requires_grad, model.parameters())), lr=1e-1, momentum=0.9, dampening=0.5, weight_decay=1e-4)
                                                                                                                                              ^
TabError: inconsistent use of tabs and spaces in indentation
Starting training
************ Running optimization 1
Current learning rate: 0.1
[1:0.10] loss: 4.661
[1:0.20] loss: 4.560
[1:0.30] loss: 4.494
[1:0.40] loss: 4.406
[1:0.50] loss: 4.334
[1:0.60] loss: 4.274
[1:0.70] loss: 4.226
[1:0.80] loss: 4.198
[1:0.90] loss: 4.130
[1:1.00] loss: 4.093
Validating ... 
The current model1:
Top 1 Accuracy: 0.0742
Top 5 Accuracy: 0.2464
Current learning rate: 0.001
[2:0.10] loss: 4.143
[2:0.20] loss: 4.009
[2:0.30] loss: 3.977
[2:0.40] loss: 3.929
[2:0.50] loss: 3.914
[2:0.60] loss: 3.874
[2:0.70] loss: 3.861
[2:0.80] loss: 3.841
[2:0.90] loss: 3.819
[2:1.00] loss: 3.787
Validating ... 
The current model2:
Top 1 Accuracy: 0.1172
Top 5 Accuracy: 0.3335
Current learning rate: 0.001
[3:0.10] loss: 3.923
[3:0.20] loss: 3.775
[3:0.30] loss: 3.753
[3:0.40] loss: 3.730
[3:0.50] loss: 3.713
[3:0.60] loss: 3.693
[3:0.70] loss: 3.678
[3:0.80] loss: 3.663
[3:0.90] loss: 3.656
[3:1.00] loss: 3.621
Validating ... 
The current model3:
Top 1 Accuracy: 0.14140001
Top 5 Accuracy: 0.38269997
Current learning rate: 0.001
[4:0.10] loss: 3.905
[4:0.20] loss: 3.654
[4:0.30] loss: 3.619
[4:0.40] loss: 3.603
[4:0.50] loss: 3.566
[4:0.60] loss: 3.569
[4:0.70] loss: 3.564
[4:0.80] loss: 3.537
[4:0.90] loss: 3.507
[4:1.00] loss: 3.495
Validating ... 
The current model4:
Top 1 Accuracy: 0.1584
Top 5 Accuracy: 0.40990007
Current learning rate: 0.001
[5:0.10] loss: 3.682
[5:0.20] loss: 3.522
[5:0.30] loss: 3.454
[5:0.40] loss: 3.458
[5:0.50] loss: 3.441
[5:0.60] loss: 3.436
[5:0.70] loss: 3.431
[5:0.80] loss: 3.398
[5:0.90] loss: 3.401
[5:1.00] loss: 3.392
Validating ... 
The current model5:
Top 1 Accuracy: 0.1739
Top 5 Accuracy: 0.43799996
Current learning rate: 0.001
  File "train.py", line 42
    lr = 1e-1
            ^
TabError: inconsistent use of tabs and spaces in indentation
Starting training
************ Running optimization 1
Current learning rate: 0.1
[1:0.10] loss: 4.666
[1:0.20] loss: 4.235
[1:0.30] loss: 4.092
[1:0.40] loss: 3.976
[1:0.50] loss: 3.914
[1:0.60] loss: 3.799
[1:0.70] loss: 3.721
[1:0.80] loss: 3.659
[1:0.90] loss: 3.581
[1:1.00] loss: 3.515
Validating ... 
The current model1:
Top 1 Accuracy: 0.1454
Top 5 Accuracy: 0.40579998
Current learning rate: 0.1
[2:0.10] loss: 3.379
[2:0.20] loss: 3.366
[2:0.30] loss: 3.306
[2:0.40] loss: 3.270
[2:0.50] loss: 3.224
[2:0.60] loss: 3.136
[2:0.70] loss: 3.091
[2:0.80] loss: 3.052
[2:0.90] loss: 2.978
[2:1.00] loss: 2.949
Validating ... 
The current model2:
Top 1 Accuracy: 0.22899997
Top 5 Accuracy: 0.5215
Current learning rate: 0.1
[3:0.10] loss: 2.881
[3:0.20] loss: 2.854
[3:0.30] loss: 2.794
[3:0.40] loss: 2.791
[3:0.50] loss: 2.753
[3:0.60] loss: 2.701
[3:0.70] loss: 2.715
[3:0.80] loss: 2.684
[3:0.90] loss: 2.631
[3:1.00] loss: 2.645
Validating ... 
The current model3:
Top 1 Accuracy: 0.31669998
Top 5 Accuracy: 0.62569994
Current learning rate: 0.1
[4:0.10] loss: 2.510
[4:0.20] loss: 2.508
[4:0.30] loss: 2.512
[4:0.40] loss: 2.468
[4:0.50] loss: 2.475
[4:0.60] loss: 2.433
[4:0.70] loss: 2.451
[4:0.80] loss: 2.444
[4:0.90] loss: 2.442
[4:1.00] loss: 2.425
Validating ... 
The current model4:
Top 1 Accuracy: 0.33760002
Top 5 Accuracy: 0.6503999
Current learning rate: 0.1
[5:0.10] loss: 2.251
[5:0.20] loss: 2.278
[5:0.30] loss: 2.272
[5:0.40] loss: 2.281
[5:0.50] loss: 2.263
[5:0.60] loss: 2.290
[5:0.70] loss: 2.268
[5:0.80] loss: 2.241
[5:0.90] loss: 2.198
[5:1.00] loss: 2.211
Validating ... 
The current model5:
Top 1 Accuracy: 0.36539996
Top 5 Accuracy: 0.6742
Current learning rate: 0.1
[6:0.10] loss: 2.068
[6:0.20] loss: 2.074
[6:0.30] loss: 2.073
[6:0.40] loss: 2.113
[6:0.50] loss: 2.110
[6:0.60] loss: 2.052
[6:0.70] loss: 2.083
[6:0.80] loss: 2.080
[6:0.90] loss: 2.078
[6:1.00] loss: 2.094
Validating ... 
The current model6:
Top 1 Accuracy: 0.3799
Top 5 Accuracy: 0.6924
Current learning rate: 0.1
[7:0.10] loss: 1.880
[7:0.20] loss: 1.905
[7:0.30] loss: 1.908
[7:0.40] loss: 1.904
[7:0.50] loss: 1.898
[7:0.60] loss: 1.950
[7:0.70] loss: 1.965
[7:0.80] loss: 1.946
[7:0.90] loss: 1.940
[7:1.00] loss: 1.955
Validating ... 
The current model7:
Top 1 Accuracy: 0.41519997
Top 5 Accuracy: 0.7314
Current learning rate: 0.1
[8:0.10] loss: 1.676
[8:0.20] loss: 1.712
[8:0.30] loss: 1.741
[8:0.40] loss: 1.792
[8:0.50] loss: 1.777
[8:0.60] loss: 1.775
[8:0.70] loss: 1.787
[8:0.80] loss: 1.812
[8:0.90] loss: 1.841
[8:1.00] loss: 1.821
Validating ... 
The current model8:
Top 1 Accuracy: 0.40769997
Top 5 Accuracy: 0.72730005
Current learning rate: 0.1
[9:0.10] loss: 1.495
[9:0.20] loss: 1.546
[9:0.30] loss: 1.578
[9:0.40] loss: 1.612
[9:0.50] loss: 1.658
[9:0.60] loss: 1.614
[9:0.70] loss: 1.647
[9:0.80] loss: 1.686
[9:0.90] loss: 1.675
[9:1.00] loss: 1.668
Validating ... 
The current model9:
Top 1 Accuracy: 0.41669995
Top 5 Accuracy: 0.73090005
Current learning rate: 0.1
[10:0.10] loss: 1.334
[10:0.20] loss: 1.389
[10:0.30] loss: 1.434
[10:0.40] loss: 1.442
[10:0.50] loss: 1.495
[10:0.60] loss: 1.503
[10:0.70] loss: 1.520
[10:0.80] loss: 1.530
[10:0.90] loss: 1.528
[10:1.00] loss: 1.532
Validating ... 
The current model10:
Top 1 Accuracy: 0.4318
Top 5 Accuracy: 0.7336
Current learning rate: 0.1
[11:0.10] loss: 1.129
[11:0.20] loss: 1.198
[11:0.30] loss: 1.231
[11:0.40] loss: 1.318
[11:0.50] loss: 1.327
[11:0.60] loss: 1.384
[11:0.70] loss: 1.365
[11:0.80] loss: 1.387
[11:0.90] loss: 1.367
[11:1.00] loss: 1.380
Validating ... 
The current model11:
Top 1 Accuracy: 0.40769994
Top 5 Accuracy: 0.72150004
Current learning rate: 0.1
[12:0.10] loss: 0.941
[12:0.20] loss: 0.997
[12:0.30] loss: 1.068
[12:0.40] loss: 1.105
[12:0.50] loss: 1.143
[12:0.60] loss: 1.184
[12:0.70] loss: 1.243
[12:0.80] loss: 1.251
[12:0.90] loss: 1.286
[12:1.00] loss: 1.254
Validating ... 
The current model12:
Top 1 Accuracy: 0.3987
Top 5 Accuracy: 0.71029997
Current learning rate: 0.1
[13:0.10] loss: 0.775
[13:0.20] loss: 0.793
[13:0.30] loss: 0.867
[13:0.40] loss: 0.959
[13:0.50] loss: 1.009
[13:0.60] loss: 1.067
[13:0.70] loss: 1.074
[13:0.80] loss: 1.081
[13:0.90] loss: 1.124
[13:1.00] loss: 1.146
Validating ... 
The current model13:
Top 1 Accuracy: 0.4023
Top 5 Accuracy: 0.7114999
Current learning rate: 0.1
[14:0.10] loss: 0.634
[14:0.20] loss: 0.620
[14:0.30] loss: 0.704
[14:0.40] loss: 0.765
[14:0.50] loss: 0.835
[14:0.60] loss: 0.918
[14:0.70] loss: 0.951
[14:0.80] loss: 0.999
[14:0.90] loss: 0.990
[14:1.00] loss: 0.985
Validating ... 
The current model14:
Top 1 Accuracy: 0.36599994
Top 5 Accuracy: 0.68160003
Current learning rate: 0.1
[15:0.10] loss: 0.523
[15:0.20] loss: 0.514
[15:0.30] loss: 0.569
[15:0.40] loss: 0.671
[15:0.50] loss: 0.742
[15:0.60] loss: 0.799
[15:0.70] loss: 0.823
[15:0.80] loss: 0.905
[15:0.90] loss: 0.884
[15:1.00] loss: 0.914
Validating ... 
The current model15:
Top 1 Accuracy: 0.38619998
Top 5 Accuracy: 0.6843
Current learning rate: 0.1
[16:0.10] loss: 0.465
[16:0.20] loss: 0.406
[16:0.30] loss: 0.467
[16:0.40] loss: 0.511
[16:0.50] loss: 0.578
[16:0.60] loss: 0.630
[16:0.70] loss: 0.703
[16:0.80] loss: 0.747
[16:0.90] loss: 0.797
[16:1.00] loss: 0.813
Validating ... 
The current model16:
Top 1 Accuracy: 0.3768
Top 5 Accuracy: 0.6805001
Current learning rate: 0.1
[17:0.10] loss: 0.394
[17:0.20] loss: 0.351
[17:0.30] loss: 0.379
[17:0.40] loss: 0.412
[17:0.50] loss: 0.485
[17:0.60] loss: 0.525
[17:0.70] loss: 0.551
[17:0.80] loss: 0.670
[17:0.90] loss: 0.688
[17:1.00] loss: 0.722
Validating ... 
The current model17:
Top 1 Accuracy: 0.38199997
Top 5 Accuracy: 0.679
Current learning rate: 0.1
[18:0.10] loss: 0.340
[18:0.20] loss: 0.296
[18:0.30] loss: 0.318
[18:0.40] loss: 0.366
[18:0.50] loss: 0.403
[18:0.60] loss: 0.452
[18:0.70] loss: 0.520
[18:0.80] loss: 0.562
[18:0.90] loss: 0.593
[18:1.00] loss: 0.619
Validating ... 
The current model18:
Top 1 Accuracy: 0.3737
Top 5 Accuracy: 0.6825
Current learning rate: 0.1
[19:0.10] loss: 0.270
[19:0.20] loss: 0.288
[19:0.30] loss: 0.285
[19:0.40] loss: 0.294
[19:0.50] loss: 0.363
[19:0.60] loss: 0.398
[19:0.70] loss: 0.509
[19:0.80] loss: 0.533
[19:0.90] loss: 0.539
[19:1.00] loss: 0.600
Validating ... 
The current model19:
Top 1 Accuracy: 0.37350002
Top 5 Accuracy: 0.6734
Current learning rate: 0.1
[20:0.10] loss: 0.265
[20:0.20] loss: 0.247
[20:0.30] loss: 0.245
[20:0.40] loss: 0.258
[20:0.50] loss: 0.295
[20:0.60] loss: 0.331
[20:0.70] loss: 0.375
[20:0.80] loss: 0.428
[20:0.90] loss: 0.456
[20:1.00] loss: 0.532
Validating ... 
The current model20:
Top 1 Accuracy: 0.36209995
Top 5 Accuracy: 0.65349996
************ Running optimization 2
Current learning rate: 0.1
Traceback (most recent call last):
  File "train.py", line 179, in <module>
    correct = pred.eq(target.view(1, -1).expand_as(pred))
  File "train.py", line 72, in run
    top1_dic = {}
UnboundLocalError: local variable 'scheduler' referenced before assignment
  File "train.py", line 84
    if scheduler:
                ^
TabError: inconsistent use of tabs and spaces in indentation
Starting training
************ Running optimization 2
Current learning rate: 1e-07
Traceback (most recent call last):
  File "train.py", line 192, in <module>
    run(i)
  File "train.py", line 84, in run
    if scheduler:
UnboundLocalError: local variable 'scheduler' referenced before assignment
Starting training
************ Running optimization 2
Current learning rate: 1e-07
[1:0.00] loss: 4.856
[1:0.00] loss: 4.826
[1:0.00] loss: 4.692
[1:0.00] loss: 4.846
[1:0.01] loss: 4.763
[1:0.01] loss: 4.830
[1:0.01] loss: 4.922
[1:0.01] loss: 4.884
[1:0.01] loss: 4.846
[1:0.01] loss: 4.753
[1:0.01] loss: 4.709
[1:0.01] loss: 4.695
[1:0.01] loss: 4.772
[1:0.01] loss: 4.874
[1:0.01] loss: 4.795
[1:0.02] loss: 4.846
[1:0.02] loss: 4.679
[1:0.02] loss: 4.894
[1:0.02] loss: 4.881
[1:0.02] loss: 4.914
[1:0.02] loss: 4.881
[1:0.02] loss: 4.755
[1:0.02] loss: 4.888
[1:0.02] loss: 4.828
[1:0.03] loss: 4.665
[1:0.03] loss: 4.860
[1:0.03] loss: 4.735
[1:0.03] loss: 4.823
[1:0.03] loss: 4.815
[1:0.03] loss: 4.793
[1:0.03] loss: 4.762
[1:0.03] loss: 4.804
[1:0.03] loss: 4.694
[1:0.03] loss: 4.737
[1:0.04] loss: 4.879
[1:0.04] loss: 4.871
[1:0.04] loss: 4.932
[1:0.04] loss: 4.903
[1:0.04] loss: 4.861
[1:0.04] loss: 4.791
[1:0.04] loss: 4.805
[1:0.04] loss: 4.826
[1:0.04] loss: 4.816
[1:0.04] loss: 4.815
[1:0.04] loss: 4.743
[1:0.05] loss: 4.885
[1:0.05] loss: 4.806
[1:0.05] loss: 4.859
[1:0.05] loss: 4.831
[1:0.05] loss: 4.794
[1:0.05] loss: 4.794
[1:0.05] loss: 4.732
[1:0.05] loss: 4.777
[1:0.05] loss: 4.808
[1:0.06] loss: 4.761
[1:0.06] loss: 4.799
[1:0.06] loss: 4.708
[1:0.06] loss: 4.712
[1:0.06] loss: 4.730
[1:0.06] loss: 4.697
[1:0.06] loss: 4.763
[1:0.06] loss: 4.796
[1:0.06] loss: 4.801
[1:0.06] loss: 4.759
[1:0.07] loss: 4.747
[1:0.07] loss: 4.713
[1:0.07] loss: 4.594
[1:0.07] loss: 4.694
[1:0.07] loss: 4.699
[1:0.07] loss: 4.656
[1:0.07] loss: 4.624
[1:0.07] loss: 4.631
[1:0.07] loss: 4.682
[1:0.07] loss: 4.656
[1:0.07] loss: 4.674
[1:0.08] loss: 4.829
[1:0.08] loss: 4.805
[1:0.08] loss: 4.961
[1:0.08] loss: 4.897
[1:0.08] loss: 5.669
[1:0.08] loss: 5.720
[1:0.08] loss: 6.325
[1:0.08] loss: 6.312
[1:0.08] loss: 7.001
[1:0.09] loss: 5.812
[1:0.09] loss: 5.682
[1:0.09] loss: 5.368
[1:0.09] loss: 5.688
[1:0.09] loss: 7.332
[1:0.09] loss: 5.633
[1:0.09] loss: 5.448
[1:0.09] loss: 5.172
[1:0.09] loss: 5.367
[1:0.09] loss: 4.905
[1:0.10] loss: 5.020
[1:0.10] loss: 4.635
[1:0.10] loss: 4.868
[1:0.10] loss: 4.865
[1:0.10] loss: 4.984
[1:0.10] loss: 4.783
[1:0.10] loss: 5.515
[1:0.10] loss: 4.887
[1:0.10] loss: 5.231
[1:0.10] loss: 4.966
[1:0.10] loss: 4.926
[1:0.11] loss: 5.487
[1:0.11] loss: 5.090
[1:0.11] loss: 5.556
[1:0.11] loss: 28.656
[1:0.11] loss: 259.484
[1:0.11] loss: 170.439
[1:0.11] loss: 3209.834
[1:0.11] loss: 85926.211
[1:0.11] loss: 2433702.000
[1:0.12] loss: 79381280.000
[1:0.12] loss: 6209729536.000
[1:0.12] loss: 249667813376.000
[1:0.12] loss: 12678408962048.000
[1:0.12] loss: 5168767654952960.000
[1:0.12] loss: 1108918018471100416.000
[1:0.12] loss: 3403942039425908736.000
[1:0.12] loss: 15231376739532800.000
[1:0.12] loss: 360278177664204800.000
[1:0.12] loss: 1650424801132544.000
[1:0.12] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nanException ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f59191b80b8>>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 399, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 378, in _shutdown_workers
    self.worker_result_queue.get()
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py", line 337, in get
    return _ForkingPickler.loads(res)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 151, in rebuild_storage_fd
    fd = df.detach()
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 493, in Client
    answer_challenge(c, authkey)
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 737, in answer_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer

[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.51] loss: nan
[1:0.51] loss: nan
[1:0.51] loss: nan
[1:0.51] loss: nan
[1:0.51] loss: nan
[1:0.51] loss: nan
[1:0.51] loss: nan
[1:0.51] loss: nan
[1:0.51] loss: nan
[1:0.51] loss: nan
[1:0.52] loss: nan
[1:0.52] loss: nan
[1:0.52] loss: nan
[1:0.52] loss: nan
[1:0.52] loss: nan
[1:0.52] loss: nan
[1:0.52] loss: nan
[1:0.52] loss: nan
[1:0.52] loss: nan
[1:0.52] loss: nan
[1:0.53] loss: nan
[1:0.53] loss: nan
[1:0.53] loss: nan
[1:0.53] loss: nan
[1:0.53] loss: nan
[1:0.53] loss: nan
[1:0.53] loss: nan
[1:0.53] loss: nan
[1:0.53] loss: nan
[1:0.53] loss: nan
[1:0.54] loss: nan
[1:0.54] loss: nan
[1:0.54] loss: nan
[1:0.54] loss: nan
[1:0.54] loss: nan
[1:0.54] loss: nan
[1:0.54] loss: nan
[1:0.54] loss: nan
[1:0.54] loss: nan
[1:0.54] loss: nan
[1:0.55] loss: nan
[1:0.55] loss: nan
[1:0.55] loss: nan
[1:0.55] loss: nan
[1:0.55] loss: nan
[1:0.55] loss: nan
[1:0.55] loss: nan
[1:0.55] loss: nan
[1:0.55] loss: nan
[1:0.55] loss: nan
[1:0.56] loss: nan
[1:0.56] loss: nan
[1:0.56] loss: nan
[1:0.56] loss: nan
[1:0.56] loss: nan
[1:0.56] loss: nan
[1:0.56] loss: nan
[1:0.56] loss: nan
[1:0.56] loss: nan
[1:0.56] loss: nan
[1:0.56] loss: nan
[1:0.57] loss: nan
[1:0.57] loss: nan
[1:0.57] loss: nan
[1:0.57] loss: nan
[1:0.57] loss: nan
Traceback (most recent call last):
  File "train.py", line 193, in <module>
    run(i)
  File "train.py", line 104, in run
    optimizer.step()
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/optim/sgd.py", line 107, in step
    p.data.add_(-group['lr'], d_p)
RuntimeError: value cannot be converted to type float without overflow: -398107170553504709826491858081542045696.000000
Starting training
************ Running optimization 3
Current learning rate: 0.1
0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

[1:0.10] loss: 4.538
0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

[1:0.20] loss: 4.123
0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

[1:0.30] loss: 3.983
0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

[1:0.40] loss: 3.855
0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

[1:0.50] loss: 3.767
0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

[1:0.60] loss: 3.662
0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

[1:0.70] loss: 3.576
0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

[1:0.80] loss: 3.502
0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

[1:0.90] loss: 3.450
0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

0.1

[1:1.00] loss: 3.374
0.1

Validating ... 
The current model1:
Top 1 Accuracy: 0.1515
Top 5 Accuracy: 0.4103
Current learning rate: 0.1
Starting training
************ Running optimization 3
Current learning rate: 0.1
[1:0.10] loss: 4.542
[1:0.20] loss: 4.142
[1:0.30] loss: 4.004
[1:0.40] loss: 3.889
[1:0.50] loss: 3.775
[1:0.60] loss: 3.671
[1:0.70] loss: 3.604
[1:0.80] loss: 3.507
[1:0.90] loss: 3.439
[1:1.00] loss: 3.366
Validating ... 
The current model1:
Top 1 Accuracy: 0.1715
Top 5 Accuracy: 0.42760003
Current learning rate: 0.1
[2:0.10] loss: 3.286
[2:0.20] loss: 3.233
[2:0.30] loss: 3.191
[2:0.40] loss: 3.147
[2:0.50] loss: 3.084
[2:0.60] loss: 3.078
[2:0.70] loss: 3.036
[2:0.80] loss: 3.025
[2:0.90] loss: 2.957
[2:1.00] loss: 2.927
Validating ... 
The current model2:
Top 1 Accuracy: 0.261
Top 5 Accuracy: 0.56400007
Current learning rate: 0.1
[3:0.10] loss: 2.835
[3:0.20] loss: 2.805
[3:0.30] loss: 2.775
[3:0.40] loss: 2.763
[3:0.50] loss: 2.744
[3:0.60] loss: 2.715
[3:0.70] loss: 2.700
[3:0.80] loss: 2.681
[3:0.90] loss: 2.671
[3:1.00] loss: 2.666
Validating ... 
The current model3:
Top 1 Accuracy: 0.30759996
Top 5 Accuracy: 0.6203
Current learning rate: 0.1
[4:0.10] loss: 2.517
[4:0.20] loss: 2.496
[4:0.30] loss: 2.484
[4:0.40] loss: 2.501
[4:0.50] loss: 2.509
[4:0.60] loss: 2.478
[4:0.70] loss: 2.514
[4:0.80] loss: 2.464
[4:0.90] loss: 2.432
[4:1.00] loss: 2.429
Validating ... 
The current model4:
Top 1 Accuracy: 0.30409998
Top 5 Accuracy: 0.6162
Current learning rate: 0.1
[5:0.10] loss: 2.276
[5:0.20] loss: 2.298
[5:0.30] loss: 2.273
[5:0.40] loss: 2.285
[5:0.50] loss: 2.274
[5:0.60] loss: 2.278
[5:0.70] loss: 2.257
[5:0.80] loss: 2.253
[5:0.90] loss: 2.291
[5:1.00] loss: 2.268
Validating ... 
The current model5:
Top 1 Accuracy: 0.3133
Top 5 Accuracy: 0.61899996
Current learning rate: 0.1
[6:0.10] loss: 2.045
[6:0.20] loss: 2.057
[6:0.30] loss: 2.105
[6:0.40] loss: 2.071
[6:0.50] loss: 2.089
[6:0.60] loss: 2.088
[6:0.70] loss: 2.098
[6:0.80] loss: 2.097
[6:0.90] loss: 2.129
[6:1.00] loss: 2.086
Validating ... 
The current model6:
Top 1 Accuracy: 0.3192
Top 5 Accuracy: 0.6302999
Current learning rate: 0.1
[7:0.10] loss: 1.818
[7:0.20] loss: 1.856
[7:0.30] loss: 1.900
[7:0.40] loss: 1.893
[7:0.50] loss: 1.924
[7:0.60] loss: 1.929
[7:0.70] loss: 1.945
[7:0.80] loss: 1.934
[7:0.90] loss: 1.905
[7:1.00] loss: 1.974
Validating ... 
The current model7:
Top 1 Accuracy: 0.34329998
Top 5 Accuracy: 0.6453999
Current learning rate: 0.1
[8:0.10] loss: 1.630
[8:0.20] loss: 1.640
[8:0.30] loss: 1.690
[8:0.40] loss: 1.697
[8:0.50] loss: 1.736
[8:0.60] loss: 1.780
[8:0.70] loss: 1.771
[8:0.80] loss: 1.788
[8:0.90] loss: 1.757
[8:1.00] loss: 1.773
Validating ... 
The current model8:
Top 1 Accuracy: 0.3468
Top 5 Accuracy: 0.64519995
Current learning rate: 0.1
[9:0.10] loss: 1.364
[9:0.20] loss: 1.400
[9:0.30] loss: 1.476
[9:0.40] loss: 1.511
[9:0.50] loss: 1.550
[9:0.60] loss: 1.555
[9:0.70] loss: 1.582
[9:0.80] loss: 1.605
[9:0.90] loss: 1.624
[9:1.00] loss: 1.610
Validating ... 
The current model9:
Top 1 Accuracy: 0.31719995
Top 5 Accuracy: 0.61329997
Current learning rate: 0.1
[10:0.10] loss: 1.129
[10:0.20] loss: 1.161
[10:0.30] loss: 1.222
[10:0.40] loss: 1.296
[10:0.50] loss: 1.389
[10:0.60] loss: 1.403
[10:0.70] loss: 1.413
[10:0.80] loss: 1.414
[10:0.90] loss: 1.428
[10:1.00] loss: 1.438
Validating ... 
The current model10:
Top 1 Accuracy: 0.3312
Top 5 Accuracy: 0.6184
Current learning rate: 0.1
[11:0.10] loss: 0.897
[11:0.20] loss: 0.931
[11:0.30] loss: 0.986
[11:0.40] loss: 1.083
[11:0.50] loss: 1.150
[11:0.60] loss: 1.176
[11:0.70] loss: 1.210
[11:0.80] loss: 1.240
[11:0.90] loss: 1.285
[11:1.00] loss: 1.283
Validating ... 
The current model11:
Top 1 Accuracy: 0.31489998
Top 5 Accuracy: 0.6109
Current learning rate: 0.1
[12:0.10] loss: 0.710
[12:0.20] loss: 0.710
[12:0.30] loss: 0.761
[12:0.40] loss: 0.859
[12:0.50] loss: 0.884
[12:0.60] loss: 0.948
[12:0.70] loss: 0.992
[12:0.80] loss: 1.084
[12:0.90] loss: 1.082
[12:1.00] loss: 1.122
Validating ... 
The current model12:
Top 1 Accuracy: 0.32560003
Top 5 Accuracy: 0.63130003
Current learning rate: 0.1
[13:0.10] loss: 0.560
[13:0.20] loss: 0.528
[13:0.30] loss: 0.565
[13:0.40] loss: 0.665
[13:0.50] loss: 0.714
[13:0.60] loss: 0.735
[13:0.70] loss: 0.806
[13:0.80] loss: 0.854
[13:0.90] loss: 0.966
[13:1.00] loss: 0.982
Validating ... 
The current model13:
Top 1 Accuracy: 0.2981
Top 5 Accuracy: 0.59599996
Current learning rate: 0.1
[14:0.10] loss: 0.439
[14:0.20] loss: 0.428
[14:0.30] loss: 0.463
[14:0.40] loss: 0.464
[14:0.50] loss: 0.516
[14:0.60] loss: 0.558
[14:0.70] loss: 0.620
[14:0.80] loss: 0.677
[14:0.90] loss: 0.717
[14:1.00] loss: 0.755
Validating ... 
The current model14:
Top 1 Accuracy: 0.32309997
Top 5 Accuracy: 0.6182
Current learning rate: 0.1
[15:0.10] loss: 0.333
[15:0.20] loss: 0.297
[15:0.30] loss: 0.302
[15:0.40] loss: 0.318
[15:0.50] loss: 0.359
[15:0.60] loss: 0.422
[15:0.70] loss: 0.462
[15:0.80] loss: 0.471
[15:0.90] loss: 0.532
[15:1.00] loss: 0.581
Validating ... 
The current model15:
Top 1 Accuracy: 0.29599997
Top 5 Accuracy: 0.59190005
Current learning rate: 0.1
[16:0.10] loss: 0.262
[16:0.20] loss: 0.246
[16:0.30] loss: 0.219
[16:0.40] loss: 0.220
[16:0.50] loss: 0.262
[16:0.60] loss: 0.290
[16:0.70] loss: 0.314
[16:0.80] loss: 0.389
[16:0.90] loss: 0.407
[16:1.00] loss: 0.458
Validating ... 
The current model16:
Top 1 Accuracy: 0.32519996
Top 5 Accuracy: 0.6275
Current learning rate: 0.1
[17:0.10] loss: 0.212
[17:0.20] loss: 0.194
[17:0.30] loss: 0.200
[17:0.40] loss: 0.200
[17:0.50] loss: 0.202
[17:0.60] loss: 0.235
[17:0.70] loss: 0.247
[17:0.80] loss: 0.272
[17:0.90] loss: 0.334
[17:1.00] loss: 0.393
Validating ... 
The current model17:
Top 1 Accuracy: 0.31149998
Top 5 Accuracy: 0.6001
Current learning rate: 0.1
[18:0.10] loss: 0.178
[18:0.20] loss: 0.167
[18:0.30] loss: 0.149
[18:0.40] loss: 0.159
[18:0.50] loss: 0.159
[18:0.60] loss: 0.157
[18:0.70] loss: 0.188
[18:0.80] loss: 0.228
[18:0.90] loss: 0.220
[18:1.00] loss: 0.271
Validating ... 
The current model18:
Top 1 Accuracy: 0.33309996
Top 5 Accuracy: 0.6259999
Current learning rate: 0.1
[19:0.10] loss: 0.112
[19:0.20] loss: 0.094
[19:0.30] loss: 0.100
[19:0.40] loss: 0.099
[19:0.50] loss: 0.111
[19:0.60] loss: 0.105
[19:0.70] loss: 0.129
[19:0.80] loss: 0.132
[19:0.90] loss: 0.131
[19:1.00] loss: 0.148
Validating ... 
The current model19:
Top 1 Accuracy: 0.34530002
Top 5 Accuracy: 0.63809997
Current learning rate: 0.1
[20:0.10] loss: 0.069
[20:0.20] loss: 0.062
[20:0.30] loss: 0.053
[20:0.40] loss: 0.053
[20:0.50] loss: 0.050
[20:0.60] loss: 0.047
[20:0.70] loss: 0.048
[20:0.80] loss: 0.046
[20:0.90] loss: 0.042
[20:1.00] loss: 0.054
Validating ... 
The current model20:
Top 1 Accuracy: 0.3724
Top 5 Accuracy: 0.6699999
************ Running optimization 4
Current learning rate: 0.1
[1:0.10] loss: 4.604
[1:0.20] loss: 4.195
[1:0.30] loss: 4.034
[1:0.40] loss: 3.926
[1:0.50] loss: 3.835
[1:0.60] loss: 3.744
[1:0.70] loss: 3.609
[1:0.80] loss: 3.544
[1:0.90] loss: 3.450
[1:1.00] loss: 3.430
Validating ... 
The current model1:
Top 1 Accuracy: 0.16129999
Top 5 Accuracy: 0.43039998
Current learning rate: 0.1
[2:0.10] loss: 3.311
[2:0.20] loss: 3.264
[2:0.30] loss: 3.193
[2:0.40] loss: 3.167
[2:0.50] loss: 3.122
[2:0.60] loss: 3.057
[2:0.70] loss: 3.021
[2:0.80] loss: 2.995
[2:0.90] loss: 2.955
[2:1.00] loss: 2.887
Validating ... 
The current model2:
Top 1 Accuracy: 0.1706
Top 5 Accuracy: 0.38889995
Current learning rate: 0.1
[3:0.10] loss: 2.849
[3:0.20] loss: 2.776
[3:0.30] loss: 2.765
[3:0.40] loss: 2.740
[3:0.50] loss: 2.707
[3:0.60] loss: 2.692
[3:0.70] loss: 2.678
[3:0.80] loss: 2.646
[3:0.90] loss: 2.612
[3:1.00] loss: 2.608
Validating ... 
The current model3:
Top 1 Accuracy: 0.3081
Top 5 Accuracy: 0.6230999
Current learning rate: 0.1
[4:0.10] loss: 2.489
[4:0.20] loss: 2.496
[4:0.30] loss: 2.463
[4:0.40] loss: 2.490
[4:0.50] loss: 2.438
[4:0.60] loss: 2.421
[4:0.70] loss: 2.409
[4:0.80] loss: 2.433
[4:0.90] loss: 2.428
[4:1.00] loss: 2.386
Validating ... 
The current model4:
Top 1 Accuracy: 0.33830005
Top 5 Accuracy: 0.66019994
Current learning rate: 0.1
[5:0.10] loss: 2.213
[5:0.20] loss: 2.260
[5:0.30] loss: 2.273
[5:0.40] loss: 2.256
[5:0.50] loss: 2.255
[5:0.60] loss: 2.247
[5:0.70] loss: 2.230
[5:0.80] loss: 2.219
[5:0.90] loss: 2.240
[5:1.00] loss: 2.238
Validating ... 
The current model5:
Top 1 Accuracy: 0.38129997
Top 5 Accuracy: 0.68990016
Current learning rate: 0.1
[6:0.10] loss: 2.030
[6:0.20] loss: 2.061
[6:0.30] loss: 2.101
[6:0.40] loss: 2.061
[6:0.50] loss: 2.050
[6:0.60] loss: 2.072
[6:0.70] loss: 2.091
[6:0.80] loss: 2.088
[6:0.90] loss: 2.074
[6:1.00] loss: 2.083
Validating ... 
The current model6:
Top 1 Accuracy: 0.38800004
Top 5 Accuracy: 0.70419997
Current learning rate: 0.1
[7:0.10] loss: 1.853
[7:0.20] loss: 1.857
[7:0.30] loss: 1.902
[7:0.40] loss: 1.899
[7:0.50] loss: 1.915
[7:0.60] loss: 1.911
[7:0.70] loss: 1.946
[7:0.80] loss: 1.939
[7:0.90] loss: 1.913
[7:1.00] loss: 1.970
Validating ... 
The current model7:
Top 1 Accuracy: 0.4093
Top 5 Accuracy: 0.7204001
Current learning rate: 0.1
[8:0.10] loss: 1.669
[8:0.20] loss: 1.711
[8:0.30] loss: 1.724
[8:0.40] loss: 1.748
[8:0.50] loss: 1.797
[8:0.60] loss: 1.788
[8:0.70] loss: 1.795
[8:0.80] loss: 1.815
[8:0.90] loss: 1.810
[8:1.00] loss: 1.799
Validating ... 
The current model8:
Top 1 Accuracy: 0.40739998
Top 5 Accuracy: 0.7249999
Current learning rate: 0.1
[9:0.10] loss: 1.503
[9:0.20] loss: 1.525
[9:0.30] loss: 1.596
[9:0.40] loss: 1.593
[9:0.50] loss: 1.656
[9:0.60] loss: 1.614
[9:0.70] loss: 1.624
[9:0.80] loss: 1.658
[9:0.90] loss: 1.654
[9:1.00] loss: 1.678
Validating ... 
The current model9:
Top 1 Accuracy: 0.4159
Top 5 Accuracy: 0.7259999
Current learning rate: 0.1
[10:0.10] loss: 1.276
[10:0.20] loss: 1.348
[10:0.30] loss: 1.421
[10:0.40] loss: 1.426
[10:0.50] loss: 1.471
[10:0.60] loss: 1.480
[10:0.70] loss: 1.540
[10:0.80] loss: 1.548
[10:0.90] loss: 1.523
[10:1.00] loss: 1.524
Validating ... 
The current model10:
Top 1 Accuracy: 0.41419998
Top 5 Accuracy: 0.7228001
Current learning rate: 0.1
[11:0.10] loss: 1.113
[11:0.20] loss: 1.144
[11:0.30] loss: 1.217
[11:0.40] loss: 1.258
[11:0.50] loss: 1.330
[11:0.60] loss: 1.334
[11:0.70] loss: 1.363
[11:0.80] loss: 1.393
[11:0.90] loss: 1.420
[11:1.00] loss: 1.367
Validating ... 
The current model11:
Top 1 Accuracy: 0.41220006
Top 5 Accuracy: 0.7243
Current learning rate: 0.1
[12:0.10] loss: 0.945
[12:0.20] loss: 0.959
[12:0.30] loss: 1.037
[12:0.40] loss: 1.101
[12:0.50] loss: 1.173
[12:0.60] loss: 1.180
[12:0.70] loss: 1.202
[12:0.80] loss: 1.233
[12:0.90] loss: 1.235
[12:1.00] loss: 1.264
Validating ... 
The current model12:
Top 1 Accuracy: 0.4076
Top 5 Accuracy: 0.7099999
Current learning rate: 0.1
[13:0.10] loss: 0.756
[13:0.20] loss: 0.757
[13:0.30] loss: 0.848
[13:0.40] loss: 0.944
[13:0.50] loss: 0.988
[13:0.60] loss: 1.037
[13:0.70] loss: 1.062
[13:0.80] loss: 1.091
[13:0.90] loss: 1.103
[13:1.00] loss: 1.174
Validating ... 
The current model13:
Top 1 Accuracy: 0.40010002
Top 5 Accuracy: 0.7041
Current learning rate: 0.1
[14:0.10] loss: 0.637
[14:0.20] loss: 0.624
[14:0.30] loss: 0.682
[14:0.40] loss: 0.761
[14:0.50] loss: 0.852
[14:0.60] loss: 0.918
[14:0.70] loss: 0.917
[14:0.80] loss: 0.971
[14:0.90] loss: 1.002
[14:1.00] loss: 1.005
Validating ... 
The current model14:
Top 1 Accuracy: 0.40219998
Top 5 Accuracy: 0.6975
Current learning rate: 0.1
[15:0.10] loss: 0.522
[15:0.20] loss: 0.512
[15:0.30] loss: 0.560
[15:0.40] loss: 0.646
[15:0.50] loss: 0.708
[15:0.60] loss: 0.781
[15:0.70] loss: 0.776
[15:0.80] loss: 0.825
[15:0.90] loss: 0.876
[15:1.00] loss: 0.899
Validating ... 
The current model15:
Top 1 Accuracy: 0.3959
Top 5 Accuracy: 0.70089996
Current learning rate: 0.1
[16:0.10] loss: 0.459
[16:0.20] loss: 0.417
[16:0.30] loss: 0.438
[16:0.40] loss: 0.491
[16:0.50] loss: 0.580
[16:0.60] loss: 0.658
[16:0.70] loss: 0.733
[16:0.80] loss: 0.749
[16:0.90] loss: 0.786
[16:1.00] loss: 0.793
Validating ... 
The current model16:
Top 1 Accuracy: 0.38750005
Top 5 Accuracy: 0.69479996
Current learning rate: 0.1
[17:0.10] loss: 0.396
[17:0.20] loss: 0.348
[17:0.30] loss: 0.367
[17:0.40] loss: 0.417
[17:0.50] loss: 0.451
[17:0.60] loss: 0.542
[17:0.70] loss: 0.622
[17:0.80] loss: 0.648
[17:0.90] loss: 0.674
[17:1.00] loss: 0.705
Validating ... 
The current model17:
Top 1 Accuracy: 0.38079995
Top 5 Accuracy: 0.68749994
Current learning rate: 0.1
[18:0.10] loss: 0.339
[18:0.20] loss: 0.301
[18:0.30] loss: 0.313
[18:0.40] loss: 0.355
[18:0.50] loss: 0.390
[18:0.60] loss: 0.425
[18:0.70] loss: 0.490
[18:0.80] loss: 0.556
[18:0.90] loss: 0.583
[18:1.00] loss: 0.636
Validating ... 
The current model18:
Top 1 Accuracy: 0.3625
Top 5 Accuracy: 0.6639999
Current learning rate: 0.1
[19:0.10] loss: 0.316
[19:0.20] loss: 0.273
[19:0.30] loss: 0.266
[19:0.40] loss: 0.293
[19:0.50] loss: 0.335
[19:0.60] loss: 0.402
[19:0.70] loss: 0.435
[19:0.80] loss: 0.484
[19:0.90] loss: 0.534
[19:1.00] loss: 0.560
Validating ... 
The current model19:
Top 1 Accuracy: 0.35860002
Top 5 Accuracy: 0.66830003
Current learning rate: 0.1
[20:0.10] loss: 0.265
[20:0.20] loss: 0.215
[20:0.30] loss: 0.223
[20:0.40] loss: 0.240
[20:0.50] loss: 0.272
[20:0.60] loss: 0.327
[20:0.70] loss: 0.359
[20:0.80] loss: 0.419
[20:0.90] loss: 0.498
[20:1.00] loss: 0.511
Validating ... 
The current model20:
Top 1 Accuracy: 0.3761
Top 5 Accuracy: 0.6739
************ Running optimization 5
Current learning rate: 0.1
[1:0.10] loss: 4.722
[1:0.20] loss: 4.325
[1:0.30] loss: 4.140
[1:0.40] loss: 4.039
[1:0.50] loss: 3.935
[1:0.60] loss: 3.849
[1:0.70] loss: 3.766
[1:0.80] loss: 3.680
[1:0.90] loss: 3.579
[1:1.00] loss: 3.496
Validating ... 
The current model1:
Top 1 Accuracy: 0.15439999
Top 5 Accuracy: 0.42580003
Current learning rate: 0.1
[2:0.10] loss: 3.445
[2:0.20] loss: 3.360
[2:0.30] loss: 3.286
[2:0.40] loss: 3.244
[2:0.50] loss: 3.180
[2:0.60] loss: 3.110
[2:0.70] loss: 3.097
[2:0.80] loss: 3.056
[2:0.90] loss: 3.015
[2:1.00] loss: 2.957
Validating ... 
The current model2:
Top 1 Accuracy: 0.2324
Top 5 Accuracy: 0.52919996
Current learning rate: 0.1
[3:0.10] loss: 2.907
[3:0.20] loss: 2.887
[3:0.30] loss: 2.819
[3:0.40] loss: 2.808
[3:0.50] loss: 2.759
[3:0.60] loss: 2.759
[3:0.70] loss: 2.720
[3:0.80] loss: 2.676
[3:0.90] loss: 2.701
[3:1.00] loss: 2.634
Validating ... 
The current model3:
Top 1 Accuracy: 0.3098
Top 5 Accuracy: 0.6186001
Current learning rate: 0.1
[4:0.10] loss: 2.523
[4:0.20] loss: 2.529
[4:0.30] loss: 2.497
[4:0.40] loss: 2.504
[4:0.50] loss: 2.466
[4:0.60] loss: 2.473
[4:0.70] loss: 2.457
[4:0.80] loss: 2.455
[4:0.90] loss: 2.435
[4:1.00] loss: 2.417
Validating ... 
The current model4:
Top 1 Accuracy: 0.35480005
Top 5 Accuracy: 0.6731
Current learning rate: 0.1
[5:0.10] loss: 2.261
[5:0.20] loss: 2.285
[5:0.30] loss: 2.274
[5:0.40] loss: 2.278
[5:0.50] loss: 2.275
[5:0.60] loss: 2.289
[5:0.70] loss: 2.237
[5:0.80] loss: 2.206
[5:0.90] loss: 2.226
[5:1.00] loss: 2.248
Validating ... 
The current model5:
Top 1 Accuracy: 0.33750004
Top 5 Accuracy: 0.6652
Current learning rate: 0.1
[6:0.10] loss: 2.068
[6:0.20] loss: 2.062
[6:0.30] loss: 2.098
[6:0.40] loss: 2.078
[6:0.50] loss: 2.082
[6:0.60] loss: 2.076
[6:0.70] loss: 2.099
[6:0.80] loss: 2.096
[6:0.90] loss: 2.095
[6:1.00] loss: 2.116
Validating ... 
The current model6:
Top 1 Accuracy: 0.38669997
Top 5 Accuracy: 0.70309997
Current learning rate: 0.1
[7:0.10] loss: 1.873
[7:0.20] loss: 1.903
[7:0.30] loss: 1.911
[7:0.40] loss: 1.920
[7:0.50] loss: 1.950
[7:0.60] loss: 1.967
[7:0.70] loss: 1.911
[7:0.80] loss: 1.961
[7:0.90] loss: 1.950
[7:1.00] loss: 1.957
Validating ... 
The current model7:
Top 1 Accuracy: 0.395
Top 5 Accuracy: 0.7161
Current learning rate: 0.1
[8:0.10] loss: 1.691
[8:0.20] loss: 1.723
[8:0.30] loss: 1.775
[8:0.40] loss: 1.796
[8:0.50] loss: 1.788
[8:0.60] loss: 1.818
[8:0.70] loss: 1.814
[8:0.80] loss: 1.815
[8:0.90] loss: 1.832
[8:1.00] loss: 1.835
Validating ... 
The current model8:
Top 1 Accuracy: 0.4057
Top 5 Accuracy: 0.7203999
Current learning rate: 0.1
[9:0.10] loss: 1.531
[9:0.20] loss: 1.609
[9:0.30] loss: 1.593
[9:0.40] loss: 1.647
[9:0.50] loss: 1.642
[9:0.60] loss: 1.695
[9:0.70] loss: 1.677
[9:0.80] loss: 1.764
[9:0.90] loss: 1.730
[9:1.00] loss: 1.719
Validating ... 
The current model9:
Top 1 Accuracy: 0.42459995
Top 5 Accuracy: 0.737
Current learning rate: 0.1
[10:0.10] loss: 1.351
[10:0.20] loss: 1.430
[10:0.30] loss: 1.527
[10:0.40] loss: 1.513
[10:0.50] loss: 1.545
[10:0.60] loss: 1.546
[10:0.70] loss: 1.576
[10:0.80] loss: 1.604
[10:0.90] loss: 1.586
[10:1.00] loss: 1.573
Validating ... 
The current model10:
Top 1 Accuracy: 0.41669998
Top 5 Accuracy: 0.7303999
Current learning rate: 0.1
[11:0.10] loss: 1.181
[11:0.20] loss: 1.275
[11:0.30] loss: 1.326
[11:0.40] loss: 1.408
[11:0.50] loss: 1.431
[11:0.60] loss: 1.423
[11:0.70] loss: 1.463
[11:0.80] loss: 1.463
[11:0.90] loss: 1.498
[11:1.00] loss: 1.505
Validating ... 
The current model11:
Top 1 Accuracy: 0.38949996
Top 5 Accuracy: 0.6986
Current learning rate: 0.1
[12:0.10] loss: 1.045
[12:0.20] loss: 1.125
[12:0.30] loss: 1.227
[12:0.40] loss: 1.275
[12:0.50] loss: 1.312
[12:0.60] loss: 1.303
[12:0.70] loss: 1.321
[12:0.80] loss: 1.352
[12:0.90] loss: 1.365
[12:1.00] loss: 1.376
Validating ... 
The current model12:
Top 1 Accuracy: 0.38399994
Top 5 Accuracy: 0.6931
Current learning rate: 0.1
[13:0.10] loss: 0.889
[13:0.20] loss: 1.012
[13:0.30] loss: 1.081
[13:0.40] loss: 1.150
[13:0.50] loss: 1.190
[13:0.60] loss: 1.201
[13:0.70] loss: 1.239
[13:0.80] loss: 1.243
[13:0.90] loss: 1.255
[13:1.00] loss: 1.258
Validating ... 
The current model13:
Top 1 Accuracy: 0.4027
Top 5 Accuracy: 0.7074999
Current learning rate: 0.1
[14:0.10] loss: 0.764
[14:0.20] loss: 0.852
[14:0.30] loss: 0.971
[14:0.40] loss: 1.033
[14:0.50] loss: 1.113
[14:0.60] loss: 1.095
[14:0.70] loss: 1.113
[14:0.80] loss: 1.135
[14:0.90] loss: 1.141
[14:1.00] loss: 1.197
Validating ... 
The current model14:
Top 1 Accuracy: 0.4114
Top 5 Accuracy: 0.7203999
Current learning rate: 0.1
[15:0.10] loss: 0.660
[15:0.20] loss: 0.718
[15:0.30] loss: 0.880
[15:0.40] loss: 0.893
[15:0.50] loss: 0.989
[15:0.60] loss: 1.018
[15:0.70] loss: 1.055
[15:0.80] loss: 1.051
[15:0.90] loss: 1.057
[15:1.00] loss: 1.077
Validating ... 
The current model15:
Top 1 Accuracy: 0.4058
Top 5 Accuracy: 0.70939994
Current learning rate: 0.1
[16:0.10] loss: 0.579
[16:0.20] loss: 0.633
[16:0.30] loss: 0.757
[16:0.40] loss: 0.817
[16:0.50] loss: 0.883
[16:0.60] loss: 0.915
[16:0.70] loss: 0.949
[16:0.80] loss: 0.979
[16:0.90] loss: 0.982
[16:1.00] loss: 0.999
Validating ... 
The current model16:
Top 1 Accuracy: 0.3976
Top 5 Accuracy: 0.6988
Current learning rate: 0.1
[17:0.10] loss: 0.512
[17:0.20] loss: 0.583
[17:0.30] loss: 0.676
[17:0.40] loss: 0.744
[17:0.50] loss: 0.788
[17:0.60] loss: 0.854
[17:0.70] loss: 0.885
[17:0.80] loss: 0.903
[17:0.90] loss: 0.900
[17:1.00] loss: 0.925
Validating ... 
The current model17:
Top 1 Accuracy: 0.3786
Top 5 Accuracy: 0.68639994
Current learning rate: 0.1
[18:0.10] loss: 0.473
[18:0.20] loss: 0.514
[18:0.30] loss: 0.584
[18:0.40] loss: 0.683
[18:0.50] loss: 0.751
[18:0.60] loss: 0.791
[18:0.70] loss: 0.786
[18:0.80] loss: 0.855
[18:0.90] loss: 0.821
[18:1.00] loss: 0.889
Validating ... 
The current model18:
Top 1 Accuracy: 0.3854
Top 5 Accuracy: 0.6814
Current learning rate: 0.1
[19:0.10] loss: 0.435
[19:0.20] loss: 0.448
[19:0.30] loss: 0.536
[19:0.40] loss: 0.624
[19:0.50] loss: 0.690
[19:0.60] loss: 0.706
[19:0.70] loss: 0.724
[19:0.80] loss: 0.752
[19:0.90] loss: 0.786
[19:1.00] loss: 0.836
Validating ... 
The current model19:
Top 1 Accuracy: 0.38149998
Top 5 Accuracy: 0.6895
Current learning rate: 0.1
[20:0.10] loss: 0.404
[20:0.20] loss: 0.390
[20:0.30] loss: 0.505
[20:0.40] loss: 0.564
[20:0.50] loss: 0.622
[20:0.60] loss: 0.664
[20:0.70] loss: 0.723
[20:0.80] loss: 0.710
[20:0.90] loss: 0.746
[20:1.00] loss: 0.774
Validating ... 
The current model20:
Top 1 Accuracy: 0.39509994
Top 5 Accuracy: 0.6974
************ Running optimization 6
Current learning rate: 0.1
[1:0.10] loss: 4.763
[1:0.20] loss: 4.359
[1:0.30] loss: 4.180
[1:0.40] loss: 4.055
[1:0.50] loss: 3.942
[1:0.60] loss: 3.878
[1:0.70] loss: 3.770
[1:0.80] loss: 3.722
[1:0.90] loss: 3.636
[1:1.00] loss: 3.573
Validating ... 
The current model1:
Top 1 Accuracy: 0.1469
Top 5 Accuracy: 0.3968
Current learning rate: 0.1
[2:0.10] loss: 3.503
[2:0.20] loss: 3.398
[2:0.30] loss: 3.369
[2:0.40] loss: 3.313
[2:0.50] loss: 3.269
[2:0.60] loss: 3.208
[2:0.70] loss: 3.165
[2:0.80] loss: 3.147
[2:0.90] loss: 3.068
[2:1.00] loss: 3.023
Validating ... 
The current model2:
Top 1 Accuracy: 0.2377
Top 5 Accuracy: 0.53309995
Current learning rate: 0.1
[3:0.10] loss: 2.961
[3:0.20] loss: 2.917
[3:0.30] loss: 2.909
[3:0.40] loss: 2.847
[3:0.50] loss: 2.847
[3:0.60] loss: 2.823
[3:0.70] loss: 2.790
[3:0.80] loss: 2.764
[3:0.90] loss: 2.769
[3:1.00] loss: 2.726
Validating ... 
The current model3:
Top 1 Accuracy: 0.2881
Top 5 Accuracy: 0.59549993
Current learning rate: 0.1
[4:0.10] loss: 2.621
[4:0.20] loss: 2.590
[4:0.30] loss: 2.596
[4:0.40] loss: 2.576
[4:0.50] loss: 2.541
[4:0.60] loss: 2.530
[4:0.70] loss: 2.500
[4:0.80] loss: 2.518
[4:0.90] loss: 2.494
[4:1.00] loss: 2.479
Validating ... 
The current model4:
Top 1 Accuracy: 0.31299996
Top 5 Accuracy: 0.6216999
Current learning rate: 0.1
[5:0.10] loss: 2.330
[5:0.20] loss: 2.322
[5:0.30] loss: 2.294
[5:0.40] loss: 2.319
[5:0.50] loss: 2.301
[5:0.60] loss: 2.323
[5:0.70] loss: 2.305
[5:0.80] loss: 2.301
[5:0.90] loss: 2.278
[5:1.00] loss: 2.283
Validating ... 
The current model5:
Top 1 Accuracy: 0.3422
Top 5 Accuracy: 0.64159995
Current learning rate: 0.1
[6:0.10] loss: 2.051
[6:0.20] loss: 2.101
[6:0.30] loss: 2.112
[6:0.40] loss: 2.121
[6:0.50] loss: 2.100
[6:0.60] loss: 2.063
[6:0.70] loss: 2.134
[6:0.80] loss: 2.082
[6:0.90] loss: 2.097
[6:1.00] loss: 2.100
Validating ... 
The current model6:
Top 1 Accuracy: 0.3802
Top 5 Accuracy: 0.6938
Current learning rate: 0.1
[7:0.10] loss: 1.834
[7:0.20] loss: 1.854
[7:0.30] loss: 1.915
[7:0.40] loss: 1.903
[7:0.50] loss: 1.894
[7:0.60] loss: 1.882
[7:0.70] loss: 1.928
[7:0.80] loss: 1.909
[7:0.90] loss: 1.906
[7:1.00] loss: 1.919
Validating ... 
The current model7:
Top 1 Accuracy: 0.38729998
Top 5 Accuracy: 0.69350004
Current learning rate: 0.1
[8:0.10] loss: 1.597
[8:0.20] loss: 1.649
[8:0.30] loss: 1.680
[8:0.40] loss: 1.696
[8:0.50] loss: 1.698
[8:0.60] loss: 1.736
[8:0.70] loss: 1.730
[8:0.80] loss: 1.734
[8:0.90] loss: 1.701
[8:1.00] loss: 1.726
Validating ... 
The current model8:
Top 1 Accuracy: 0.3814
Top 5 Accuracy: 0.69909996
Current learning rate: 0.1
[9:0.10] loss: 1.298
[9:0.20] loss: 1.403
[9:0.30] loss: 1.438
[9:0.40] loss: 1.500
[9:0.50] loss: 1.499
[9:0.60] loss: 1.522
[9:0.70] loss: 1.518
[9:0.80] loss: 1.516
[9:0.90] loss: 1.547
[9:1.00] loss: 1.574
Validating ... 
The current model9:
Top 1 Accuracy: 0.40200007
Top 5 Accuracy: 0.70369995
Current learning rate: 0.1
[10:0.10] loss: 1.096
[10:0.20] loss: 1.128
[10:0.30] loss: 1.206
[10:0.40] loss: 1.232
[10:0.50] loss: 1.272
[10:0.60] loss: 1.264
[10:0.70] loss: 1.324
[10:0.80] loss: 1.326
[10:0.90] loss: 1.330
[10:1.00] loss: 1.331
Validating ... 
The current model10:
Top 1 Accuracy: 0.38949996
Top 5 Accuracy: 0.6894
Current learning rate: 0.1
[11:0.10] loss: 0.807
[11:0.20] loss: 0.875
[11:0.30] loss: 0.941
[11:0.40] loss: 1.006
[11:0.50] loss: 1.021
[11:0.60] loss: 1.052
[11:0.70] loss: 1.103
[11:0.80] loss: 1.095
[11:0.90] loss: 1.105
[11:1.00] loss: 1.114
Validating ... 
The current model11:
Top 1 Accuracy: 0.38539997
Top 5 Accuracy: 0.68410003
Current learning rate: 0.1
[12:0.10] loss: 0.592
[12:0.20] loss: 0.631
[12:0.30] loss: 0.706
[12:0.40] loss: 0.787
[12:0.50] loss: 0.799
[12:0.60] loss: 0.857
[12:0.70] loss: 0.862
[12:0.80] loss: 0.887
[12:0.90] loss: 0.916
[12:1.00] loss: 0.950
Validating ... 
The current model12:
Top 1 Accuracy: 0.38600004
Top 5 Accuracy: 0.6842998
Current learning rate: 0.1
[13:0.10] loss: 0.419
[13:0.20] loss: 0.438
[13:0.30] loss: 0.511
[13:0.40] loss: 0.595
[13:0.50] loss: 0.585
[13:0.60] loss: 0.641
[13:0.70] loss: 0.675
[13:0.80] loss: 0.700
[13:0.90] loss: 0.701
[13:1.00] loss: 0.746
Validating ... 
The current model13:
Top 1 Accuracy: 0.3775
Top 5 Accuracy: 0.6819001
Current learning rate: 0.1
[14:0.10] loss: 0.332
[14:0.20] loss: 0.327
[14:0.30] loss: 0.373
[14:0.40] loss: 0.453
[14:0.50] loss: 0.468
[14:0.60] loss: 0.476
[14:0.70] loss: 0.517
[14:0.80] loss: 0.535
[14:0.90] loss: 0.550
[14:1.00] loss: 0.573
Validating ... 
The current model14:
Top 1 Accuracy: 0.36749995
Top 5 Accuracy: 0.66510004
Current learning rate: 0.1
[15:0.10] loss: 0.239
[15:0.20] loss: 0.226
[15:0.30] loss: 0.273
[15:0.40] loss: 0.272
[15:0.50] loss: 0.337
[15:0.60] loss: 0.374
[15:0.70] loss: 0.383
[15:0.80] loss: 0.400
[15:0.90] loss: 0.416
[15:1.00] loss: 0.454
Validating ... 
The current model15:
Top 1 Accuracy: 0.3755
Top 5 Accuracy: 0.66950005
Current learning rate: 0.1
[16:0.10] loss: 0.195
[16:0.20] loss: 0.188
[16:0.30] loss: 0.203
[16:0.40] loss: 0.216
[16:0.50] loss: 0.240
[16:0.60] loss: 0.284
[16:0.70] loss: 0.319
[16:0.80] loss: 0.324
[16:0.90] loss: 0.318
[16:1.00] loss: 0.366
Validating ... 
The current model16:
Top 1 Accuracy: 0.37050003
Top 5 Accuracy: 0.6636999
Current learning rate: 0.1
[17:0.10] loss: 0.160
[17:0.20] loss: 0.152
[17:0.30] loss: 0.170
[17:0.40] loss: 0.188
[17:0.50] loss: 0.192
[17:0.60] loss: 0.224
[17:0.70] loss: 0.240
[17:0.80] loss: 0.261
[17:0.90] loss: 0.256
[17:1.00] loss: 0.275
Validating ... 
The current model17:
Top 1 Accuracy: 0.38339993
Top 5 Accuracy: 0.68159986
Current learning rate: 0.1
[18:0.10] loss: 0.135
[18:0.20] loss: 0.127
[18:0.30] loss: 0.124
[18:0.40] loss: 0.141
[18:0.50] loss: 0.161
[18:0.60] loss: 0.163
[18:0.70] loss: 0.196
[18:0.80] loss: 0.203
[18:0.90] loss: 0.212
[18:1.00] loss: 0.247
Validating ... 
The current model18:
Top 1 Accuracy: 0.36969998
Top 5 Accuracy: 0.6703
Current learning rate: 0.1
[19:0.10] loss: 0.128
[19:0.20] loss: 0.123
[19:0.30] loss: 0.109
[19:0.40] loss: 0.117
[19:0.50] loss: 0.124
[19:0.60] loss: 0.139
[19:0.70] loss: 0.139
[19:0.80] loss: 0.161
[19:0.90] loss: 0.184
[19:1.00] loss: 0.204
Validating ... 
The current model19:
Top 1 Accuracy: 0.37589997
Top 5 Accuracy: 0.6703
Current learning rate: 0.1
[20:0.10] loss: 0.105
[20:0.20] loss: 0.088
[20:0.30] loss: 0.085
[20:0.40] loss: 0.100
[20:0.50] loss: 0.103
[20:0.60] loss: 0.113
[20:0.70] loss: 0.113
[20:0.80] loss: 0.123
[20:0.90] loss: 0.132
[20:1.00] loss: 0.147
Validating ... 
The current model20:
Top 1 Accuracy: 0.37879997
Top 5 Accuracy: 0.67829996
Training terminated
  File "train.py", line 60
    num_epochs = 10
                  ^
TabError: inconsistent use of tabs and spaces in indentation
Starting training
************ Running optimization 7
Current learning rate: 0.1
[1:0.10] loss: 4.606
[1:0.20] loss: 4.186
[1:0.30] loss: 4.043
[1:0.40] loss: 3.937
[1:0.50] loss: 3.847
[1:0.60] loss: 3.713
[1:0.70] loss: 3.629
[1:0.80] loss: 3.560
[1:0.90] loss: 3.504
[1:1.00] loss: 3.430
Validating ... 
The current model1:
Top 1 Accuracy: 0.16
Top 5 Accuracy: 0.41770005
Current learning rate: 0.1
[2:0.10] loss: 3.338
[2:0.20] loss: 3.290
[2:0.30] loss: 3.245
[2:0.40] loss: 3.195
[2:0.50] loss: 3.137
[2:0.60] loss: 3.127
[2:0.70] loss: 3.088
[2:0.80] loss: 2.982
[2:0.90] loss: 2.939
[2:1.00] loss: 2.949
Validating ... 
The current model2:
Top 1 Accuracy: 0.22819997
Top 5 Accuracy: 0.5362
Current learning rate: 0.1
[3:0.10] loss: 2.845
[3:0.20] loss: 2.800
[3:0.30] loss: 2.798
[3:0.40] loss: 2.779
[3:0.50] loss: 2.713
[3:0.60] loss: 2.698
[3:0.70] loss: 2.679
[3:0.80] loss: 2.654
[3:0.90] loss: 2.637
[3:1.00] loss: 2.593
Validating ... 
The current model3:
Top 1 Accuracy: 0.3032
Top 5 Accuracy: 0.61759996
Current learning rate: 0.1
[4:0.10] loss: 2.478
[4:0.20] loss: 2.474
[4:0.30] loss: 2.486
[4:0.40] loss: 2.482
[4:0.50] loss: 2.464
[4:0.60] loss: 2.445
[4:0.70] loss: 2.415
[4:0.80] loss: 2.395
[4:0.90] loss: 2.408
[4:1.00] loss: 2.421
Validating ... 
The current model4:
Top 1 Accuracy: 0.3525
Top 5 Accuracy: 0.669
Current learning rate: 0.1
[5:0.10] loss: 2.219
[5:0.20] loss: 2.261
[5:0.30] loss: 2.259
[5:0.40] loss: 2.265
[5:0.50] loss: 2.266
[5:0.60] loss: 2.239
[5:0.70] loss: 2.251
[5:0.80] loss: 2.225
[5:0.90] loss: 2.224
[5:1.00] loss: 2.216
Validating ... 
The current model5:
Top 1 Accuracy: 0.36720002
Top 5 Accuracy: 0.6834
Current learning rate: 0.1
[6:0.10] loss: 2.031
[6:0.20] loss: 2.054
[6:0.30] loss: 2.069
[6:0.40] loss: 2.080
[6:0.50] loss: 2.110
[6:0.60] loss: 2.087
[6:0.70] loss: 2.086
[6:0.80] loss: 2.085
[6:0.90] loss: 2.094
[6:1.00] loss: 2.058
Validating ... 
The current model6:
Top 1 Accuracy: 0.37309998
Top 5 Accuracy: 0.6921
Current learning rate: 0.1
[7:0.10] loss: 1.873
[7:0.20] loss: 1.898
[7:0.30] loss: 1.917
[7:0.40] loss: 1.928
[7:0.50] loss: 1.910
[7:0.60] loss: 1.949
[7:0.70] loss: 1.940
[7:0.80] loss: 1.926
[7:0.90] loss: 1.936
[7:1.00] loss: 1.937
Validating ... 
The current model7:
Top 1 Accuracy: 0.4021
Top 5 Accuracy: 0.71800005
Current learning rate: 0.1
[8:0.10] loss: 1.689
[8:0.20] loss: 1.723
[8:0.30] loss: 1.747
[8:0.40] loss: 1.792
[8:0.50] loss: 1.802
[8:0.60] loss: 1.775
[8:0.70] loss: 1.802
[8:0.80] loss: 1.801
[8:0.90] loss: 1.820
[8:1.00] loss: 1.819
Validating ... 
The current model8:
Top 1 Accuracy: 0.39880002
Top 5 Accuracy: 0.70839995
Current learning rate: 0.1
[9:0.10] loss: 1.527
[9:0.20] loss: 1.555
[9:0.30] loss: 1.595
[9:0.40] loss: 1.612
[9:0.50] loss: 1.644
[9:0.60] loss: 1.658
[9:0.70] loss: 1.659
[9:0.80] loss: 1.662
[9:0.90] loss: 1.686
[9:1.00] loss: 1.670
Validating ... 
The current model9:
Top 1 Accuracy: 0.4177
Top 5 Accuracy: 0.7223999
Current learning rate: 0.1
[10:0.10] loss: 1.286
[10:0.20] loss: 1.366
[10:0.30] loss: 1.422
[10:0.40] loss: 1.475
[10:0.50] loss: 1.464
[10:0.60] loss: 1.515
[10:0.70] loss: 1.506
[10:0.80] loss: 1.501
[10:0.90] loss: 1.556
[10:1.00] loss: 1.508
Validating ... 
The current model10:
Top 1 Accuracy: 0.38990003
Top 5 Accuracy: 0.69839996
************ Running optimization 8
Current learning rate: 1e-07
[1:0.00] loss: 4.793
[1:0.00] loss: 4.927
[1:0.00] loss: 4.775
[1:0.00] loss: 4.891
[1:0.01] loss: 4.847
[1:0.01] loss: 4.725
[1:0.01] loss: 4.821
[1:0.01] loss: 4.874
[1:0.01] loss: 4.887
[1:0.01] loss: 4.737
[1:0.01] loss: 4.789
[1:0.01] loss: 4.793
[1:0.01] loss: 4.776
[1:0.01] loss: 4.861
[1:0.01] loss: 4.879
[1:0.02] loss: 4.894
[1:0.02] loss: 4.744
[1:0.02] loss: 4.815
[1:0.02] loss: 4.739
[1:0.02] loss: 4.798
[1:0.02] loss: 4.769
[1:0.02] loss: 4.777
[1:0.02] loss: 4.950
[1:0.02] loss: 4.829
[1:0.03] loss: 4.722
[1:0.03] loss: 4.814
[1:0.03] loss: 4.813
[1:0.03] loss: 4.896
[1:0.03] loss: 4.845
[1:0.03] loss: 4.706
[1:0.03] loss: 4.842
[1:0.03] loss: 4.797
[1:0.03] loss: 4.844
[1:0.03] loss: 4.751
[1:0.04] loss: 4.797
[1:0.04] loss: 4.789
[1:0.04] loss: 4.741
[1:0.04] loss: 4.809
[1:0.04] loss: 4.729
[1:0.04] loss: 4.800
[1:0.04] loss: 4.805
[1:0.04] loss: 4.779
[1:0.04] loss: 4.750
[1:0.04] loss: 4.763
[1:0.04] loss: 4.737
[1:0.05] loss: 4.654
[1:0.05] loss: 4.782
[1:0.05] loss: 4.727
[1:0.05] loss: 4.676
[1:0.05] loss: 4.621
[1:0.05] loss: 4.638
[1:0.05] loss: 4.631
[1:0.05] loss: 4.671
[1:0.05] loss: 4.623
[1:0.06] loss: 4.601
[1:0.06] loss: 4.618
[1:0.06] loss: 4.641
[1:0.06] loss: 4.607
[1:0.06] loss: 4.616
[1:0.06] loss: 4.718
[1:0.06] loss: 4.630
[1:0.06] loss: 4.668
[1:0.06] loss: 4.893
[1:0.06] loss: 4.902
[1:0.07] loss: 4.856
[1:0.07] loss: 5.010
[1:0.07] loss: 4.929
[1:0.07] loss: 5.204
[1:0.07] loss: 5.331
[1:0.07] loss: 5.471
[1:0.07] loss: 5.495
[1:0.07] loss: 6.210
[1:0.07] loss: 5.540
[1:0.07] loss: 5.667
[1:0.07] loss: 6.860
[1:0.08] loss: 5.629
[1:0.08] loss: 6.200
[1:0.08] loss: 7.282
[1:0.08] loss: 5.639
[1:0.08] loss: 5.841
[1:0.08] loss: 5.491
[1:0.08] loss: 5.704
[1:0.08] loss: 5.353
[1:0.08] loss: 6.395
[1:0.09] loss: 5.432
[1:0.09] loss: 4.714
[1:0.09] loss: 4.999
[1:0.09] loss: 4.670
[1:0.09] loss: 4.841
[1:0.09] loss: 4.665
[1:0.09] loss: 5.258
[1:0.09] loss: 4.618
[1:0.09] loss: 4.667
[1:0.09] loss: 4.880
[1:0.10] loss: 4.663
[1:0.10] loss: 4.726
[1:0.10] loss: 4.981
[1:0.10] loss: 4.984
[1:0.10] loss: 5.106
[1:0.10] loss: 4.967
[1:0.10] loss: 5.234
[1:0.10] loss: 6.112
[1:0.10] loss: 7.725
[1:0.10] loss: 21.991
[1:0.10] loss: 352.083
[1:0.11] loss: 66213.758
[1:0.11] loss: 18489772.000
[1:0.11] loss: 414729312.000
[1:0.11] loss: 55223181312.000
[1:0.11] loss: 43490268413952.000
[1:0.11] loss: 14041416321728512.000
[1:0.11] loss: 189926236967927808.000
[1:0.11] loss: 104026633487329001472.000
[1:0.11] loss: 18934283867521024.000
[1:0.12] loss: 4226103400988672.000
[1:0.12] loss: 977901587294846976.000
[1:0.12] loss: nan
[1:0.12] loss: nan
[1:0.12] loss: nan
[1:0.12] loss: nan
[1:0.12] loss: nan
[1:0.12] loss: nan
[1:0.12] loss: nan
[1:0.12] loss: nan
[1:0.12] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.13] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.14] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.15] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.16] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.17] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.18] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.19] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.20] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.21] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.22] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.23] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.24] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.25] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.26] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.27] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.28] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.29] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.30] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.31] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.32] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.33] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.34] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.35] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.36] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.37] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.38] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.39] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.40] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nanException ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fab455c1198>>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 399, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 378, in _shutdown_workers
    self.worker_result_queue.get()
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py", line 337, in get
    return _ForkingPickler.loads(res)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 151, in rebuild_storage_fd
    fd = df.detach()
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 493, in Client
    answer_challenge(c, authkey)
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 732, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError: 

[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.41] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.42] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.43] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.44] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.45] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.46] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.47] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.48] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.49] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.50] loss: nan
[1:0.51] loss: nan
[1:0.51] loss: nan
Traceback (most recent call last):
  File "train.py", line 196, in <module>
    run(i)
  File "train.py", line 105, in run
    optimizer.step()
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/optim/sgd.py", line 107, in step
    p.data.add_(-group['lr'], d_p)
RuntimeError: value cannot be converted to type float without overflow: -346736850452543066618677899014558449664.000000

Starting training
************ Running optimization 8
Current learning rate: 0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[1:0.10] loss: 4.616
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[1:0.20] loss: 4.215
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[1:0.30] loss: 4.072
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[1:0.40] loss: 3.969
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[1:0.50] loss: 3.856
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[1:0.60] loss: 3.736
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[1:0.70] loss: 3.651
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[1:0.80] loss: 3.611
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[1:0.90] loss: 3.480
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[1:1.00] loss: 3.411
0.1
Validating ... 
The current model1:
Top 1 Accuracy: 0.1539
Top 5 Accuracy: 0.41140002
Current learning rate: 0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[2:0.10] loss: 3.343
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[2:0.20] loss: 3.287
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[2:0.30] loss: 3.221
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[2:0.40] loss: 3.174
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[2:0.50] loss: 3.097
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[2:0.60] loss: 3.077
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[2:0.70] loss: 3.011
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[2:0.80] loss: 3.009
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[2:0.90] loss: 2.933
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[2:1.00] loss: 2.894
0.1
Validating ... 
The current model2:
Top 1 Accuracy: 0.2573
Top 5 Accuracy: 0.5643
Current learning rate: 0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[3:0.10] loss: 2.806
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[3:0.20] loss: 2.761
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[3:0.30] loss: 2.759
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[3:0.40] loss: 2.740
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[3:0.50] loss: 2.684
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[3:0.60] loss: 2.687
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[3:0.70] loss: 2.648
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[3:0.80] loss: 2.630
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[3:0.90] loss: 2.595
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[3:1.00] loss: 2.602
0.1
Validating ... 
The current model3:
Top 1 Accuracy: 0.322
Top 5 Accuracy: 0.6378
Current learning rate: 0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[4:0.10] loss: 2.487
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[4:0.20] loss: 2.493
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[4:0.30] loss: 2.459
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[4:0.40] loss: 2.460
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[4:0.50] loss: 2.418
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[4:0.60] loss: 2.459
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[4:0.70] loss: 2.396
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[4:0.80] loss: 2.393
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[4:0.90] loss: 2.407
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[4:1.00] loss: 2.356
0.1
Validating ... 
The current model4:
Top 1 Accuracy: 0.35549998
Top 5 Accuracy: 0.6668
Current learning rate: 0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[5:0.10] loss: 2.248
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[5:0.20] loss: 2.317
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[5:0.30] loss: 2.280
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[5:0.40] loss: 2.268
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[5:0.50] loss: 2.247
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[5:0.60] loss: 2.246
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[5:0.70] loss: 2.244
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[5:0.80] loss: 2.253
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[5:0.90] loss: 2.200
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[5:1.00] loss: 2.171
0.1
Validating ... 
The current model5:
Top 1 Accuracy: 0.3723
Top 5 Accuracy: 0.6824
Current learning rate: 0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[6:0.10] loss: 2.079
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[6:0.20] loss: 2.046
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[6:0.30] loss: 2.077
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[6:0.40] loss: 2.075
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[6:0.50] loss: 2.094
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[6:0.60] loss: 2.102
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[6:0.70] loss: 2.067
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[6:0.80] loss: 2.043
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[6:0.90] loss: 2.066
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[6:1.00] loss: 2.091
0.1
Validating ... 
The current model6:
Top 1 Accuracy: 0.38889998
Top 5 Accuracy: 0.7102001
Current learning rate: 0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[7:0.10] loss: 1.861
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[7:0.20] loss: 1.877
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[7:0.30] loss: 1.926
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[7:0.40] loss: 1.953
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[7:0.50] loss: 1.940
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[7:0.60] loss: 1.924
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[7:0.70] loss: 1.957
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[7:0.80] loss: 1.943
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[7:0.90] loss: 1.940
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
[7:1.00] loss: 1.943
0.1
Validating ... 
The current model7:
Top 1 Accuracy: 0.39770004
Top 5 Accuracy: 0.7073999
Current learning rate: 0.1
